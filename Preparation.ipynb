{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martin0310/Data-Mining/blob/main/Preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIGMcXe8x-cm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd  \n",
        "import io  \n",
        "import requests  \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvSCkLX4D9oq"
      },
      "outputs": [],
      "source": [
        "# add submit.csv\n",
        "url = \"http://140.123.173.10/truefake_dataset/submit.csv\"  \n",
        "s = requests.get(url).content  \n",
        "submit_csv = pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
        "submit_csv.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbOskTH0EHBh"
      },
      "outputs": [],
      "source": [
        "# add train.csv\n",
        "# label: 1 --> unreliable, 0 --> reliable\n",
        "url = \"http://140.123.173.10/truefake_dataset/train.csv\"  \n",
        "s = requests.get(url).content  \n",
        "# train_csv = pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
        "# train 60%\n",
        "# train_test 40%\n",
        "get_rows = 4532\n",
        "train_csv = pd.read_csv(io.StringIO(s.decode('utf-8')), nrows=get_rows)\n",
        "skip_list = list(range(1, 4533)) + list(range(4533 + (get_rows * 4) // 6, train_csv.shape[0]))\n",
        "train_test_csv = pd.read_csv(io.StringIO(s.decode('utf-8')), skiprows=[1, 2], nrows=(get_rows * 4) // 6)\n",
        "# print(train_csv)\n",
        "# print(train_test_csv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uob7f7GbOqPZ"
      },
      "outputs": [],
      "source": [
        "pp = list(range(111))\n",
        "print(pp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LteI4bgDndy"
      },
      "outputs": [],
      "source": [
        "# <class 'pandas.core.series.Series'> to list --> 2D\n",
        "def pcss2List2D(pcss, pcss_len):\n",
        "  ret_list = []\n",
        "  for i in range(pcss_len):\n",
        "    ret_list.append([])\n",
        "    tmp_list = eval(pcss[i])\n",
        "    for j in range(len(tmp_list)):\n",
        "      ret_list[i].append(tmp_list[j])\n",
        "  return ret_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iy1-rWDSERex"
      },
      "outputs": [],
      "source": [
        "# # add test.csv\n",
        "# url = \"http://140.123.173.10/truefake_dataset/test.csv\"  \n",
        "# s = requests.get(url).content  \n",
        "# test_csv = pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
        "# test_csv.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfsiAXl5ExLv"
      },
      "outputs": [],
      "source": [
        "# train_csv preprocess...\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "from nltk.cluster import KMeansClusterer\n",
        "import jieba\n",
        "import numpy as np\n",
        "from sklearn import cluster\n",
        "from sklearn import metrics\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArlcelL91t9E"
      },
      "outputs": [],
      "source": [
        "# add stopword.txt\n",
        "# url = \"https://gist.githubusercontent.com/larsyencken/1440509/raw/53273c6c202b35ef00194d06751d8ef630e53df2/stopwords.txt\"  \n",
        "# s = requests.get(url).content  \n",
        "# stopword_arr = s.decode('utf-8').split(\"\\n\")\n",
        "# stopword_arr.pop()\n",
        "# for i in range(6):\n",
        "#   stopword_arr.pop(0)\n",
        "\n",
        "# print(stopword_arr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "tG-hhTUgp8by",
        "outputId": "b811ac7f-a2af-46b7-821b-9656dd800c3b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uN9eLQVCmjzv"
      },
      "outputs": [],
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "ps = PorterStemmer() # Stemming\n",
        "stop_words = list(set(stopwords.words('english'))) # Stopword\n",
        "stop_words_len = len(stop_words)\n",
        "print(stop_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9QI4H6C7JOw"
      },
      "outputs": [],
      "source": [
        "# function to remove stop words\n",
        "def remove_stop_words(word_list, stop_words_list):\n",
        "  for stop_words_element in stop_words_list:\n",
        "    while stop_words_element in word_list:\n",
        "      word_list.remove(stop_words_element)\n",
        "  return word_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvkaapYgrb-C"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import csv\n",
        "import time\n",
        "from progressbar import *\n",
        "# remove train_csv's stopwords\n",
        "\n",
        "train_csv_rows_count = train_csv.shape[0]\n",
        "\n",
        "# import to train2.csv\n",
        "# id, title vector, text vector, author\n",
        "\n",
        "with open('/content/train2.csv', 'w', newline='') as train2_file:\n",
        "  writer = csv.writer(train2_file)\n",
        "  writer.writerow([\"id\", \"title_vector\", \"text_vector\", \"author\", \"label\"])\n",
        "\n",
        "progress = ProgressBar()\n",
        "for i in progress(range(train_csv_rows_count)):\n",
        "  # title\n",
        "  title_arr_tmp = []\n",
        "  if not isinstance(train_csv['title'][i],type(float('nan'))):\n",
        "    title_arr_tmp = train_csv['title'][i].lower().replace(',', '').replace('?', '').replace(':', '').replace('\"', '').replace('!', '').split(' ')\n",
        "  title_arr_tmp = remove_stop_words(title_arr_tmp, stop_words)\n",
        "\n",
        "  # text\n",
        "  text_arr_tmp = []\n",
        "  if not isinstance(train_csv['text'][i],type(float('nan'))):\n",
        "    text_arr_tmp = train_csv['text'][i].lower().replace(',', '').replace('?', '').replace(':', '').replace('\"', '').replace('!', '').split(' ')\n",
        "  text_arr_tmp = remove_stop_words(text_arr_tmp, stop_words)\n",
        "\n",
        "  # author\n",
        "  author_tmp = 'UNKNOWNAUTHOR'\n",
        "  if not isinstance(train_csv['author'][i],type(float('nan'))):\n",
        "    author_tmp = train_csv['author'][i]\n",
        "\n",
        "  # label\n",
        "  label_tmp = 1\n",
        "  if not isinstance(train_csv['label'][i],type(float('nan'))):\n",
        "    label_tmp = train_csv['label'][i]\n",
        "\n",
        "  # append to file\n",
        "  with open('/content/train2.csv', 'a', newline='') as train2_file:\n",
        "    writer = csv.writer(train2_file)\n",
        "    writer.writerow([i, title_arr_tmp, text_arr_tmp, author_tmp, label_tmp])\n",
        "\n",
        "print(\"train2.csv ok\")\n",
        "\n",
        "train2_csv = pd.read_csv('train2.csv')\n",
        "print(train2_csv.head())\n",
        "print(train2_csv.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ssoy3Np0hbcm"
      },
      "outputs": [],
      "source": [
        "# calculate author's score in train_csv\n",
        "author_name_score = {}\n",
        "for i in range(train2_csv.shape[0]):\n",
        "  author_name = str(train2_csv['author'][i])\n",
        "  \n",
        "  if author_name not in author_name_score.keys():\n",
        "    author_name_score[author_name] = 0\n",
        "\n",
        "  if train2_csv['label'][i] == 1:\n",
        "    author_name_score[author_name] = author_name_score[author_name] - 0.0001\n",
        "  elif train_csv['label'][i] == 0:\n",
        "    author_name_score[author_name] = author_name_score[author_name] + 0.0001\n",
        "\n",
        "print(author_name_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPK4jyNUvGXK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "train2_title_vector_list = pcss2List2D(train2_csv['title_vector'], train2_csv.shape[0])\n",
        "train2_text_vector_list = pcss2List2D(train2_csv['text_vector'], train2_csv.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2MyxB62P_hg"
      },
      "outputs": [],
      "source": [
        "# train2 word2vec\n",
        "\n",
        "train2_title_word2vec_size = 5\n",
        "train2_text_word2vec_size = 20\n",
        "\n",
        "train2_title_word2vec_min_count = 1\n",
        "train2_text_word2vec_min_count = 1\n",
        "\n",
        "# title\n",
        "train2_title_word2vec = Word2Vec(train2_title_vector_list, size=train2_title_word2vec_size, min_count=train2_title_word2vec_min_count)\n",
        "print(\"title:\")\n",
        "# print(len(train2_title_word2vec.wv.vectors))\n",
        "# print(train2_title_word2vec.wv.vocab.keys())\n",
        "# text\n",
        "train2_text_word2vec = Word2Vec(train2_text_vector_list, size=train2_text_word2vec_size, min_count=train2_text_word2vec_min_count)\n",
        "print(\"text:\")\n",
        "# print(len(train2_text_word2vec.wv.vectors))\n",
        "# print(train2_text_word2vec.wv.vocab.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3xYUj17Z-l7"
      },
      "outputs": [],
      "source": [
        "print(len(train2_text_word2vec.wv.vectors))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHpgZrJnmqGY"
      },
      "outputs": [],
      "source": [
        "print(type(train2_text_word2vec.wv.vectors))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePxTroDL9HtR"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def identity_tokenizer(text):\n",
        "  return text\n",
        "\n",
        "# train2 tf-idf\n",
        "\n",
        "tfidf = TfidfVectorizer(tokenizer=identity_tokenizer, lowercase = False)\n",
        "# title\n",
        "train2_title_result = tfidf.fit_transform(train2_title_vector_list)\n",
        "# text\n",
        "train2_text_result = tfidf.fit_transform(train2_text_vector_list)\n",
        "\n",
        "# print(train2_title_result.todense())\n",
        "# print(train2_text_result.todense())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pA-A5WXW9lM1"
      },
      "outputs": [],
      "source": [
        "train2_title_tfidf_weighted = np.dot(train2_title_result.todense(), train2_title_word2vec.wv.vectors)\n",
        "train2_text_tfidf_weighted = np.dot(train2_text_result.todense(), train2_text_word2vec.wv.vectors)\n",
        "# print(train2_title_tfidf_weighted_list)\n",
        "# print(train2_text_tfidf_weighted_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcIOyWCE3YN6"
      },
      "outputs": [],
      "source": [
        "# save to train3.csv\n",
        "print(\"save to train3.csv\")\n",
        "with open('/content/train3.csv', 'w', newline='') as train3_file:\n",
        "  writer = csv.writer(train3_file)\n",
        "  col_list = []\n",
        "  col_list.append(\"id\")\n",
        "  for i in range(train2_title_word2vec_size):\n",
        "    col_list.append(\"title_vector_\" + str(i))\n",
        "  for i in range(train2_text_word2vec_size):\n",
        "    col_list.append(\"text_vector_\" + str(i))\n",
        "  col_list.append(\"author\")\n",
        "  col_list.append(\"label\")\n",
        "  col_list.append(\"author_score\")\n",
        "  # print(len(col_list))\n",
        "  writer.writerow(col_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tq_zY6fH3b-F"
      },
      "outputs": [],
      "source": [
        "for i in range(train2_csv.shape[0]):\n",
        "  col_list_value = []\n",
        "  col_list_value.append(i)\n",
        "  train2_title_tfidf_weighted_list = train2_title_tfidf_weighted[i].tolist()[0]\n",
        "  # print(train2_title_tfidf_weighted_list)\n",
        "  for j in range(len(train2_title_tfidf_weighted_list)):\n",
        "    col_list_value.append(train2_title_tfidf_weighted_list[j])\n",
        "  train2_text_tfidf_weighted_list = train2_text_tfidf_weighted[i].tolist()[0]\n",
        "  for j in range(len(train2_text_tfidf_weighted_list)):\n",
        "    col_list_value.append(train2_text_tfidf_weighted_list[j])\n",
        "  col_list_value.append(train2_csv['author'][i])\n",
        "  col_list_value.append(train2_csv['label'][i])\n",
        "  col_list_value.append(author_name_score[train2_csv['author'][i]])\n",
        "\n",
        "  with open('/content/train3.csv', 'a', newline='') as train3_file:\n",
        "    writer = csv.writer(train3_file)\n",
        "    writer.writerow(col_list_value)\n",
        "    # print(len(col_list_value))\n",
        "\n",
        "print(\"train3.csv ok\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7cWvLkeXjW9"
      },
      "outputs": [],
      "source": [
        "# save to train3_no_author_score.csv\n",
        "print(\"save to train3_no_author_score.csv\")\n",
        "with open('/content/train3_no_author_score.csv', 'w', newline='') as train3_file:\n",
        "  writer = csv.writer(train3_file)\n",
        "  col_list = []\n",
        "  col_list.append(\"id\")\n",
        "  for i in range(train2_title_word2vec_size):\n",
        "    col_list.append(\"title_vector_\" + str(i))\n",
        "  for i in range(train2_text_word2vec_size):\n",
        "    col_list.append(\"text_vector_\" + str(i))\n",
        "  col_list.append(\"author\")\n",
        "  col_list.append(\"label\")\n",
        "  # print(len(col_list))\n",
        "  writer.writerow(col_list)\n",
        "for i in range(train2_csv.shape[0]):\n",
        "  col_list_value = []\n",
        "  col_list_value.append(i)\n",
        "  train2_title_tfidf_weighted_list = train2_title_tfidf_weighted[i].tolist()[0]\n",
        "  # print(train2_title_tfidf_weighted_list)\n",
        "  for j in range(len(train2_title_tfidf_weighted_list)):\n",
        "    col_list_value.append(train2_title_tfidf_weighted_list[j])\n",
        "  train2_text_tfidf_weighted_list = train2_text_tfidf_weighted[i].tolist()[0]\n",
        "  for j in range(len(train2_text_tfidf_weighted_list)):\n",
        "    col_list_value.append(train2_text_tfidf_weighted_list[j])\n",
        "  col_list_value.append(train2_csv['author'][i])\n",
        "  col_list_value.append(train2_csv['label'][i])\n",
        "\n",
        "  with open('/content/train3_no_author_score.csv', 'a', newline='') as train3_file:\n",
        "    writer = csv.writer(train3_file)\n",
        "    writer.writerow(col_list_value)\n",
        "    # print(len(col_list_value))\n",
        "\n",
        "print(\"train3_no_author_score.csv ok\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Mn_TSgt3aIS"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------------\n",
        "import math\n",
        "import csv\n",
        "import time\n",
        "from progressbar import *\n",
        "# remove train_csv's stopwords\n",
        "\n",
        "train_test_csv_rows_count = train_test_csv.shape[0]\n",
        "\n",
        "# import to train_test2.csv\n",
        "# id, title vector, text vector, author\n",
        "\n",
        "with open('/content/train_test2.csv', 'w', newline='') as train_test2_file:\n",
        "  writer = csv.writer(train_test2_file)\n",
        "  writer.writerow([\"id\", \"title_vector\", \"text_vector\", \"author\", \"label\"])\n",
        "\n",
        "progress = ProgressBar()\n",
        "for i in progress(range(train_test_csv_rows_count)):\n",
        "  # title\n",
        "  title_arr_tmp = []\n",
        "  if not isinstance(train_test_csv['title'][i],type(float('nan'))):\n",
        "    title_arr_tmp = train_test_csv['title'][i].lower().replace(',', '').replace('?', '').replace(':', '').replace('\"', '').replace('!', '').split(' ')\n",
        "  title_arr_tmp = remove_stop_words(title_arr_tmp, stop_words)\n",
        "\n",
        "  # text\n",
        "  text_arr_tmp = []\n",
        "  if not isinstance(train_test_csv['text'][i],type(float('nan'))):\n",
        "    text_arr_tmp = train_test_csv['text'][i].lower().replace(',', '').replace('?', '').replace(':', '').replace('\"', '').replace('!', '').split(' ')\n",
        "  text_arr_tmp = remove_stop_words(text_arr_tmp, stop_words)\n",
        "\n",
        "  # author\n",
        "  author_tmp = 'UNKNOWNAUTHOR'\n",
        "  if not isinstance(train_test_csv['author'][i],type(float('nan'))):\n",
        "    author_tmp = train_test_csv['author'][i]\n",
        "\n",
        "  # label\n",
        "  label_tmp = 1\n",
        "  if not isinstance(train_test_csv['label'][i],type(float('nan'))):\n",
        "    label_tmp = train_test_csv['label'][i]\n",
        "\n",
        "  # append to file\n",
        "  with open('/content/train_test2.csv', 'a', newline='') as train_test2_file:\n",
        "    writer = csv.writer(train_test2_file)\n",
        "    writer.writerow([i, title_arr_tmp, text_arr_tmp, author_tmp, label_tmp])\n",
        "\n",
        "print(\"train_test2.csv ok\")\n",
        "\n",
        "train_test2_csv = pd.read_csv('train_test2.csv')\n",
        "print(train_test2_csv.head())\n",
        "print(train_test2_csv.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XB2JACzrKLX3"
      },
      "outputs": [],
      "source": [
        "train_test2_title_vector_list = pcss2List2D(train_test2_csv['title_vector'], train_test2_csv.shape[0])\n",
        "train_test2_text_vector_list = pcss2List2D(train_test2_csv['text_vector'], train_test2_csv.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJdRPAjiKqpu"
      },
      "outputs": [],
      "source": [
        "# train_test2 word2vec\n",
        "\n",
        "train_test2_title_word2vec_size = 5\n",
        "train_test2_text_word2vec_size = 5\n",
        "\n",
        "train_test2_title_word2vec_min_count = 1\n",
        "train_test2_text_word2vec_min_count = 1\n",
        "\n",
        "# title\n",
        "train_test2_title_word2vec = Word2Vec(train_test2_title_vector_list, size=train_test2_title_word2vec_size, min_count=train_test2_title_word2vec_min_count)\n",
        "print(\"title:\")\n",
        "# print(len(train_test2_title_word2vec.wv.vectors))\n",
        "# print(train_test2_title_word2vec.wv.vocab.keys())\n",
        "# text\n",
        "train_test2_text_word2vec = Word2Vec(train_test2_text_vector_list, size=train_test2_text_word2vec_size, min_count=train_test2_text_word2vec_min_count)\n",
        "print(\"text:\")\n",
        "# print(len(train_test2_text_word2vec.wv.vectors))\n",
        "# print(train_test2_text_word2vec.wv.vocab.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZTFCTR3LWfj"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def identity_tokenizer(text):\n",
        "  return text\n",
        "\n",
        "# train2 tf-idf\n",
        "\n",
        "tfidf = TfidfVectorizer(tokenizer=identity_tokenizer, lowercase = False)\n",
        "# title\n",
        "train_test2_title_result = tfidf.fit_transform(train_test2_title_vector_list)\n",
        "# text\n",
        "train_test2_text_result = tfidf.fit_transform(train_test2_text_vector_list)\n",
        "\n",
        "# print(train_test2_title_result.todense())\n",
        "# print(train_test2_text_result.todense())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hKPENOPLhXM"
      },
      "outputs": [],
      "source": [
        "train_test2_title_tfidf_weighted = np.dot(train_test2_title_result.todense(), train_test2_title_word2vec.wv.vectors)\n",
        "train_test2_text_tfidf_weighted = np.dot(train_test2_text_result.todense(), train_test2_text_word2vec.wv.vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssXJ7NwKLqkj"
      },
      "outputs": [],
      "source": [
        "# save to train_test3.csv\n",
        "print(\"save to train_test3.csv\")\n",
        "with open('/content/train_test3.csv', 'w', newline='') as train_test3_file:\n",
        "  writer = csv.writer(train_test3_file)\n",
        "  col_list = []\n",
        "  col_list.append(\"id\")\n",
        "  for i in range(train_test2_title_word2vec_size):\n",
        "    col_list.append(\"title_vector_\" + str(i))\n",
        "  for i in range(train_test2_text_word2vec_size):\n",
        "    col_list.append(\"text_vector_\" + str(i))\n",
        "  col_list.append(\"label\")\n",
        "  col_list.append(\"author_score\")\n",
        "  # print(len(col_list))\n",
        "  writer.writerow(col_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cbtbrf6bL8jQ"
      },
      "outputs": [],
      "source": [
        "for i in range(train_test2_csv.shape[0]):\n",
        "  col_list_value = []\n",
        "  col_list_value.append(i)\n",
        "  train_test2_title_tfidf_weighted_list = train_test2_title_tfidf_weighted[i].tolist()[0]\n",
        "  # print(train_test2_title_tfidf_weighted_list)\n",
        "  for j in range(len(train_test2_title_tfidf_weighted_list)):\n",
        "    col_list_value.append(train_test2_title_tfidf_weighted_list[j])\n",
        "  train_test2_text_tfidf_weighted_list = train_test2_text_tfidf_weighted[i].tolist()[0]\n",
        "  for j in range(len(train_test2_text_tfidf_weighted_list)):\n",
        "    col_list_value.append(train_test2_text_tfidf_weighted_list[j])\n",
        "\n",
        "  col_list_value.append(train_test2_csv['label'][i])\n",
        "  \n",
        "  if train_test2_csv['author'][i] in author_name_score.keys():\n",
        "    col_list_value.append(author_name_score[train_test2_csv['author'][i]])\n",
        "  else:\n",
        "    col_list_value.append(0)\n",
        "\n",
        "  with open('/content/train_test3.csv', 'a', newline='') as train_test3_file:\n",
        "    writer = csv.writer(train_test3_file)\n",
        "    writer.writerow(col_list_value)\n",
        "    # print(len(col_list_value))\n",
        "\n",
        "print(\"train_test3.csv ok\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pdUk58bOTazH"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "url = \"http://140.123.173.10/truefake_dataset/train.csv\"  \n",
        "s = requests.get(url).content  \n",
        "text = s.decode('utf-8')\n",
        "print(type(text))\n",
        "cloud = WordCloud(background_color='white').generate(text)\n",
        "cloud.to_file('/content/output.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VGbxQU_ZV8A2"
      },
      "outputs": [],
      "source": [
        "# (string, filename)\n",
        "def gen_wordcloud_to(s, f):\n",
        "  cloud = WordCloud(background_color='white').generate(s)\n",
        "  cloud.to_file(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "36MCT66AWYiz"
      },
      "outputs": [],
      "source": [
        "url = \"http://140.123.173.10/truefake_dataset/train.csv\"  \n",
        "s = requests.get(url).content  \n",
        "text = s.decode('utf-8')\n",
        "gen_wordcloud_to(text, \"/content/origin.png\")\n",
        "ttt1 = ' '.join(str(item) for innerlist in train_test2_title_vector_list for item in innerlist)\n",
        "ttt2 = ' '.join(str(item) for innerlist in train_test2_text_vector_list for item in innerlist)\n",
        "ttt = ttt1 + \" \" + ttt2\n",
        "gen_wordcloud_to(ttt, \"/content/no_stop_words.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FeYpbRVGgrBC"
      },
      "outputs": [],
      "source": [
        "lll = ' '.join(str(item) for innerlist in train_test2_title_vector_list for item in innerlist)\n",
        "print(lll)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0WxQFCTJMRX9"
      },
      "outputs": [],
      "source": [
        "# # make doc tfidf matrix to dot with word2vec matrix\n",
        "\n",
        "# # title\n",
        "# train2_title_tf_matrix = []\n",
        "# train2_title_idf_matrix = []\n",
        "# train2_title_tfidf_matrix = []\n",
        "\n",
        "# word_key_list = list(train2_title_word2vec.wv.vocab.keys())\n",
        "\n",
        "# # title tf\n",
        "# print(\"title tf\")\n",
        "# # row length: amount of documents\n",
        "# # col length: amount of kwywords\n",
        "# for i in range(train2_csv.shape[0]):\n",
        "#   if i % 1000 == 0:\n",
        "#     print(str(i / train2_csv.shape[0] * 100) + \" %\")\n",
        "#   tf_list_tmp = []\n",
        "#   for j in range(len(word_key_list)):\n",
        "#     tf_counter_tmp = 0\n",
        "#     for k in range(len(train2_title_vector_list[i])):\n",
        "#       if train2_title_vector_list[i][k].__eq__(word_key_list[j]):\n",
        "#         tf_counter_tmp = tf_counter_tmp + 1\n",
        "#     # if tf_counter_tmp > 1:\n",
        "#     #   print(word_key_list[j])\n",
        "#     tf_list_tmp.append(tf_counter_tmp)\n",
        "#   train2_title_tf_matrix.append(tf_list_tmp)\n",
        "# print(\"train2_title_tf_matrix ok\")\n",
        "# # print(train2_title_tf_matrix)\n",
        "\n",
        "# # title idf\n",
        "# print(\"title idf\")\n",
        "# # row length: 1\n",
        "# # col length: amount of kwywords\n",
        "# for i in range(len(word_key_list)):\n",
        "#   if i % 1000 == 0:\n",
        "#     print(str(i / train2_csv.shape[0] * 100) + \" %\")\n",
        "#   df_counter_tmp = 0\n",
        "#   for j in range(train2_csv.shape[0]):\n",
        "#     if word_key_list[i] in train2_title_vector_list[j]:\n",
        "#       df_counter_tmp = df_counter_tmp + 1\n",
        "#   idf_tmp = math.log(train2_csv.shape[0] / (1 + df_counter_tmp))\n",
        "#   train2_title_idf_matrix.append(idf_tmp)\n",
        "# print(\"train2_title_idf_matrix ok\")\n",
        "# # print(train2_title_idf_matrix)\n",
        "\n",
        "# # title tfidf\n",
        "# print(\"title tfidf\")\n",
        "# # row length: amount of documents\n",
        "# # col length: amount of kwywords\n",
        "# for i in range(len(train2_title_tf_matrix)):\n",
        "#   tfidf_list_tmp = []\n",
        "#   for j in range(len(train2_title_tf_matrix[i])):\n",
        "#     tfidf_tmp = train2_title_tf_matrix[i][j] * train2_title_idf_matrix[i]\n",
        "#     tfidf_list_tmp.append(tfidf_tmp)\n",
        "#   train2_title_tfidf_matrix.append(tfidf_list_tmp)\n",
        "# print(\"train2_title_tfidf_matrix ok\")\n",
        "# # print(train2_title_tfidf_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mS0szDmxWBmA"
      },
      "outputs": [],
      "source": [
        "# # text\n",
        "# train2_text_tf_matrix = []\n",
        "# train2_text_idf_matrix = []\n",
        "# train2_text_tfidf_matrix = []\n",
        "\n",
        "# word_key_list = list(train2_text_word2vec.wv.vocab.keys())\n",
        "\n",
        "# # text tf\n",
        "# print(\"text tf\")\n",
        "# # row length: amount of documents\n",
        "# # col length: amount of kwywords\n",
        "# for i in range(train2_csv.shape[0]):\n",
        "#   if i % 2 == 0:\n",
        "#     print(str(i / train2_csv.shape[0] * 100) + \" %\")\n",
        "#   tf_list_tmp = []\n",
        "#   for j in range(len(word_key_list)):\n",
        "#     tf_counter_tmp = 0\n",
        "#     for k in range(len(train2_text_vector_list[i])):\n",
        "#       if train2_text_vector_list[i][k].__eq__(word_key_list[j]):\n",
        "#         tf_counter_tmp = tf_counter_tmp + 1\n",
        "#     # if tf_counter_tmp > 1:\n",
        "#     #   print(word_key_list[j])\n",
        "#     tf_list_tmp.append(tf_counter_tmp)\n",
        "#   train2_text_tf_matrix.append(tf_list_tmp)\n",
        "# print(\"train2_text_tf_matrix ok\")\n",
        "# # print(train2_text_tf_matrix)\n",
        "\n",
        "# # text idf\n",
        "# print(\"text idf\")\n",
        "# # row length: 1\n",
        "# # col length: amount of kwywords\n",
        "# for i in range(len(word_key_list)):\n",
        "#   if i % 1000 == 0:\n",
        "#     print(str(i / len(word_key_list) * 100) + \" %\")\n",
        "#   df_counter_tmp = 0\n",
        "#   for j in range(train2_csv.shape[0]):\n",
        "#     if word_key_list[i] in train2_text_vector_list[j]:\n",
        "#       df_counter_tmp = df_counter_tmp + 1\n",
        "#   idf_tmp = math.log(train2_csv.shape[0] / (1 + df_counter_tmp))\n",
        "#   train2_text_idf_matrix.append(idf_tmp)\n",
        "# print(\"train2_text_idf_matrix ok\")\n",
        "# # print(train2_text_idf_matrix)\n",
        "\n",
        "# # text tfidf\n",
        "# print(\"text tfidf\")\n",
        "# # row length: amount of documents\n",
        "# # col length: amount of kwywords\n",
        "# for i in range(len(train2_text_tf_matrix)):\n",
        "#   tfidf_list_tmp = []\n",
        "#   for j in range(len(train2_text_tf_matrix[i])):\n",
        "#     tfidf_tmp = train2_text_tf_matrix[i][j] * train2_text_idf_matrix[i]\n",
        "#     tfidf_list_tmp.append(tfidf_tmp)\n",
        "#   train2_text_tfidf_matrix.append(tfidf_list_tmp)\n",
        "# print(\"train2_text_tfidf_matrix ok\")\n",
        "# # print(train2_text_tfidf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XcFSpMPWqCHE"
      },
      "outputs": [],
      "source": [
        "# def matrix_multiplier_2D(m1, m2):\n",
        "#   res = [[0 for x in range(len(m2[0]))] for y in range(len(m1))]\n",
        "#   # explicit for loops\n",
        "#   for i in range(len(m1)):\n",
        "#     for j in range(len(m2[0])):\n",
        "#       for k in range(len(m2)):\n",
        "#         # resulted matrix\n",
        "#         res[i][j] += m1[i][k] * m2[k][j]\n",
        "#   return res"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Preparation.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}